# 8강 정리 RNN,LSTM,GRU

역전파를 구할때 W를 계속 가중치를 곱해주니 기울기 폭발 사례가 발생

예) I am a man 이라는 단어를 예측할때 I * W + I*W*am*W 이란 방식으로 가중치가 계속 증가하기 떄문에 기울기 폭발이 발생

임시 방편 해결 방법 클리핑 최대 값이 증가할때 그냥 잘라내기 특정 기울기 이상 증가하면 잘라내기

문제 정리

1. 기울기 폭발 문제:
    - 맞습니다. 긴 시퀀스에서 역전파를 수행할 때 가중치가 반복적으로 곱해지면서 기울기가 폭발적으로 커질 수 있습니다.
2. 수식 표현:
    - 실제로는 "I * W + I*W*am*W"와 같은 방식으로 직접 곱해지지는 않습니다.
    - RNN에서는 각 시간 단계마다 동일한 가중치 행렬을 사용하며, 활성화 함수를 통과합니다.
3. 정확한 메커니즘:
    - RNN에서 t 시점의 은닉 상태는 대략 h_t = tanh(W_hh * h_(t-1) + W_xh * x_t) 형태입니다.
    - 역전파 시 이 식의 미분이 연쇄적으로 곱해지면서 기울기 폭발이나 소실이 발생할 수 있습니다.
4. 기울기 소실 문제도 존재:
    - 기울기 폭발뿐만 아니라, 가중치가 1보다 작은 경우 기울기가 0에 가까워지는 기울기 소실 문제도 발생할 수 있습니다.

이러한 문제들을 해결하기 위해 LSTM, GRU 등의 구조나 gradient clipping 같은 기법이 사용됩니다.

### LSTM(Long Short-Term Memory)은 기울기 소실과 폭발 문제를 해결하기 위해 설계된 RNN의 변형

주요 특징을 정리하면:

1. 게이트 메커니즘:
    - Forget gate: 이전 정보를 얼마나 잊을지 결정
    - Input gate: 새로운 정보를 얼마나 저장할지 결정
    - Output gate: 현재 셀 상태의 어떤 부분을 출력할지 결정
2. 셀 상태(Cell state):
    - 정보를 장기간 저장하는 컨베이어 벨트 역할
    - 선형적인 연산으로 정보 흐름을 유지
3. 장기 의존성 해결:
    - 중요한 정보를 오랜 시간 동안 기억할 수 있음
    - 불필요한 정보는 forget gate를 통해 제거
4. 기울기 문제 완화:
    - 셀 상태를 통해 기울기가 끊김 없이 흐를 수 있는 경로 제공
    - 이로 인해 장기 의존성 학습이 가능해짐

LSTM은 이러한 구조를 통해 기존 RNN의 한계를 극복하고, 장기 의존성을 효과적으로 학습

GRU(Gated Recurrent Unit)는 LSTM을 간소화한 버전



### Seq2Seq : seq2seq는 아키텍처(방식)이고, 내부에 적용하는 모델은 여러 가지

1. Seq2Seq의 목적:
    - 입력 시퀀스를 받아 전체 출력 시퀀스를 생성합니다.
    - 예: 문장 번역, 텍스트 요약 등에 사용됩니다.
2. 구조:
    - 인코더: 입력 시퀀스를 고정 길이의 컨텍스트 벡터로 압축합니다.
    - 디코더: 이 컨텍스트 벡터를 사용해 출력 시퀀스를 생성합니다.
3. 작동 방식:
    - 인코더는 입력 시퀀스의 모든 단어를 처리하여 최종 상태를 생성합니다.
    - 디코더는 이 최종 상태를 초기 상태로 사용하여 출력 시퀀스를 생성합니다.
    - 각 단계에서 디코더는 이전 출력과 현재 상태를 사용하여 다음 단어를 예측합니다.
4. 정보 전달:
    - 인코더의 최종 상태가 전체 입력 문장의 정보를 포함합니다.
    - 디코더는 이 정보를 바탕으로 출력 시퀀스를 순차적으로 생성합니다.